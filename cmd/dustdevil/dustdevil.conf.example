# dustdevil example configuration
#
# log settings
log: {
        # directory to store logs in. current directory if unset
        path: '/srv/dustdevil/instance/log'
        # log file name
        file: 'dustdevil.log'
        # reopen logfile on signal USR2
        rotate.on.usr2: true
}

# zookeeper settings
zookeeper: {
        # publish offset updates every commit.ms
        commit.ms: 2000
        # servers and chroot offset specification
        connect.string: 'zk-server01:2181,zk-server02:2181/chroot/kafka'
        # ignore the zookeeper stored kafka consumer offset
        reset.offset.on.startup: false
}

# kafka settings
kafka: {
        # consumergroup to join
        consumer.group.name: 'dustdevil_instance'
        # topics to consumer
        consumer.topics: 'mistral'
        # consume from oldest or newest offset
        consumer.offset.strategy: 'newest'
        # keepalive interval in ms
        keepalive.ms: 4200
}

# legacy metrics settings
legacy: {
        # path to the metric socket
        socket.path: '/run/dustdevil.seqpacket'
        # print metrics on STDERR
        metrics.debug.stderr: false
        # print metrics every n seconds
        metrics.debug.frequency.seconds: 30
}

# redis settings
redis: {
    # redis server address
    connect: 'localhost:6379'
    # redis database to store records in
    db: '0'
    # redis connect password
    password: 'sikrit'
    # expire time for cache records
    cache.timeout.seconds: 1800
}

# profile server settings
eyewall: {
    host: 'localhost'
    port: '7777'
    path: 'api/v1/configuration'
}

# misc settings
misc: {
        # produce metrics on metricsocket
        produce.metrics: true
        # instance name, will be included in metrics if set
        instance.name: ''
}

# dustdevil application settings
dustdevil: {
        # internal handler queue length
        handler.queue.length: 16
        # uri of the statistics API to forward events to
        api.endpoint: 'http://localhost:8088/metrics'
        # api retry count
        post.request.retry.count: 4
        # minimum wait time between retries
        retry.min.wait.time.ms: 500
        # maximum wait time between retries
        retry.max.wait.time.ms: 4200
        # post request timeout
        request.timeout.ms: 1600
        # do not forward string metrics
        strip.string.metrics: true
        # limit of concurrent POST requests, 0 for unlimited
        post.request.concurrency.limit: 128
        # set to true if the api endpoint is ElasticSearch
        api.endpoint.is.elasticsearch: false
        # set to either 'batch' or 'split' depending on the content
        # of the consumed kafka topic
        input.format: split
}
